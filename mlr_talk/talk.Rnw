\documentclass[10pt]{beamer}

\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{array}
\usepackage{adjustbox}
\usepackage{xspace}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,backgrounds,fit,positioning,chains,shadows,decorations.pathmorphing,decorations.pathreplacing,matrix}
\usepackage{csquotes}
\usepackage{booktabs}
\usepackage{wasysym}
\usepackage[binary-units=true]{siunitx}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{dsfont}

\definecolor{tugreen}{cmyk}{0.57, 0, 1.00, 0}
\definecolor{tugreen1}{cmyk}{0.57, 0, 1.00, 0}
\definecolor{tugreen2}{HTML}{667E4D}
\definecolor{tugreen3}{HTML}{72A544}
\definecolor{tugreen4}{HTML}{3A472E}

\usecolortheme{dove}
\usetheme{boxes}
\usefonttheme{structuresmallcapsserif}
\newenvironment{whiteframe}
{
 \usebackgroundtemplate{}
 \begin{frame}
}
{
 \end{frame}
}

\usetikzlibrary{shapes,matrix,positioning,chains,arrows,shadows,decorations.pathmorphing,fit,backgrounds}
\setbeamercolor{itemize item}{fg=tugreen1}
\setbeamercolor{itemize subitem}{fg=tugreen1}
\setbeamertemplate{itemize item}[square]
\setbeamertemplate{footline}[frame number]
\beamertemplatenavigationsymbolsempty

\title{Machine Learning in R: Package \texttt{mlr}}
\logo{\includegraphics[scale=0.05]{fig/mlr.png}}
\author{Jakob Richter, TU Dortmund}
\titlegraphic{\includegraphics[height=.3\textheight]{fig/mlr.png}}
\date{}

\newcommand{\norm}[2][\relax]{\ifx#1\relax\ensuremath{\left\Vert#2\right\Vert}\else\ensuremath{\left\Vert#2\right\Vert_{#1}}\fi}
\newcommand{\ind}{\mathds{1}}
\newcommand{\pred}[1]{\ind\left(#1\right)}
\newcommand{\abs}[1]{\ensuremath{\left| #1 \right|}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\texttt{#1}}
\newcommand{\tarrow}{\textcolor{tugreen1}{{\ding{212}}}\xspace}

% suppress frame numbering, so noframenumbering works
% \setbeamertemplate{frametitle continuation}
%   \begin{frame}[containsverbatim,allowframebreaks,noframenumbering]

\newenvironment{vframe}
{
  \begin{frame}[containsverbatim]
}
{
 \end{frame}
}

\newenvironment{vbframe}
{
  \begin{frame}[containsverbatim,allowframebreaks]
}
{
 \end{frame}
}

\newenvironment{blocki*}
{
  \begin{block}{}\begin{itemize}
}
{
\end{itemize}\end{block}
}

\newenvironment{blocki}[1]
{
  \begin{block}{#1}\begin{itemize}
}
{
\end{itemize}\end{block}
}

\newcommand{\oneliner}[1]{\begin{block}{}\begin{center}\begin{Large}#1\end{Large}\end{center}\end{block}}


\renewcommand<>{\sout}[1]{
  \only#2{\beameroriginal{\sout}{#1}}
  \invisible#2{#1}
}


\AtBeginSection{\frame{\sectionpage}}


\begin{document}
% \usebackgroundtemplate{
%   \begin{tikzpicture}
%     \shade [inner color = white, outer color = gray!30, opacity = 0.8] (\paperwidth,\paperheight) rectangle (0,0);
%     \shade [inner color = white, outer color = gray!10, opacity=.05] (\paperwidth/2,\paperheight/2) circle (3);
%   \end{tikzpicture}
% }

<<opts,include=FALSE,cache=FALSE>>=
library(knitr)
library(BBmisc)
library(mlr)
library(ggplot2)
library(parallelMap)
library(tikzDevice)
library(data.table)
library(gridExtra)
library(survMisc)
options(width = 70)
configureMlr(show.info = FALSE)
configureMlr(show.learner.output = FALSE)
OPENML_EVAL = TRUE

knit_hooks$set(document = function(x) {
  # silence xcolor
  x = sub('\\usepackage[]{color}', '\\usepackage{xcolor}', x, fixed = TRUE)
  # add an noindent after hooks -> remove blank line
  x = gsub('(\\\\end\\{knitrout\\}[\n]+)', '\\1\\\\noindent ', x)
  x
})

opts_chunk$set(
   cache = TRUE,
   tidy = FALSE,
#   dev = 'tikz',
   external = TRUE,
   fig.align = "center",
   size = "scriptsize",
   stop = TRUE,
   fig.width = 9 * 0.8,
   fig.height = 6 * 0.8,
   small.mar = TRUE,
   prompt = TRUE
)
@

%% PART I
\begin{frame}
  \titlepage
\end{frame}

\begin{vframe}{About}
  \begin{itemize}
    \item Project home page\\
    \oneliner{\url{https://github.com/mlr-org/mlr}}
      \begin{itemize}
        \item \textbf{Tutorial} including many examples
        \item R documentation
        \item Ask questions in the github issue tracker or stackoverflow
        % \item Wiki page for this tutorial (slides, hands on solutions, \ldots)
      \end{itemize}
    \item 8-10 main developers, quite a few contributors, 5 GSOC projects sinse 2015
    \item About 20K lines of code, 8K lines of unit tests
    % \item If you do not have \pkg{mlr} installed yet, please do so (see wiki page)
      % \item Same for \pkg{OpenML} (not on CRAN, you'll need \pkg{devools}):
% <<openml-install,eval=FALSE>>=
% install.packages("devtools")
% devtools::install_github("openml/r")
% @
  \end{itemize}
\end{vframe}

\begin{vframe}{Supervised Classification tasks}
<<classification-task-plot,echo=FALSE,fig.height=4>>=
set.seed(1)
df = data.frame(x = c(rnorm(10, mean = 3), rnorm(10, mean = 5)), y = runif(10), class = rep(c("a", "b"), each = 10))
ggplot(df, aes(x = x, y = y, shape = class, color = class)) + geom_point(size = 3) + geom_vline(xintercept = 4, linetype = "longdash")
@
\structure{Goal}: Predict a class (or membership probabilities)
\end{vframe}

\begin{vframe}{Supervised Regression tasks}
<<regression-task-plot,echo=FALSE,fig.height=4>>=
set.seed(1)
f = function(x) 0.5 * x^2 + x + sin(x)
x = runif(40, min = -3, max = 3)
y = f(x) + rnorm(40)
df = data.frame(x = x, y = y)
ggplot(df, aes(x, y)) + geom_point(size = 3) + stat_function(fun = f, color = "#FF9999", size = 2)
@
\structure{Goal}: Predict a continuous output
\end{vframe}

\begin{vframe}{Supervised Survival tasks}
<<survial-task-plot,echo=FALSE,fig.height=4>>=
set.seed(1)
sf = survival:::survfit(survival::Surv(time, status) ~ rx, data = rats)
survMisc:::autoplot.survfit(sf, title = "", xLab = "Time", yLab = expression(hat(S)(t)), survLineSize = 1.5)$plot
@
\structure{Goal}: Predict a survival function $\hat{S}(t)$, i.e.\ the probability to survive to time point~$t$
\end{vframe}


\begin{vframe}{Unsupervised Cluster tasks}
<<cluster-task-plot,echo=FALSE,fig.height=4>>=
df = iris
m = as.matrix(cbind(df$Petal.Length, df$Petal.Width),ncol=2)
cl = (kmeans(m,3))
df$cluster = factor(cl$cluster)
centers = as.data.frame(cl$centers)
ggplot(data=df, aes(x=Petal.Length, y=Petal.Width, color=cluster )) +
 geom_point() +
 geom_point(data=centers, aes(x=V1,y=V2, color='Center')) +
 geom_point(data=centers, aes(x=V1,y=V2, color='Center'), size=52, alpha=.3) +
 theme(legend.position="none")
@
\structure{Goal}: Group data into similar clusters (or estimate fuzzy membership probabilities)
\end{vframe}

\begin{vframe}{Motivation}
  \begin{blocki}{The good news}
  \item CRAN serves hundreds of packages for machine learning
    % (cf.\ CRAN task view machine learning)
  \item Often compliant to the unwritten interface definition:
<<model-standard,eval=FALSE>>=
model = fit(target ~ ., data = train.data, ...)
predictions = predict(model, newdata = test.data, ...)
@
  \end{blocki}

  \begin{blocki}{The bad news}
    \item Some packages API is \enquote{just different}
    \item Functionality is always package or model-dependent, even though the procedure might be general
    \item No meta-information available or buried in docs
      % (sometimes not documented at all)
    % \item Many packages require the user to \enquote{guess} good hyperparameters
    \item Result: lengthy, tedious and error-prone code
  \end{blocki}
  \oneliner{Our goal: A domain-specific language for many machine learning concepts!}
\end{vframe}

\begin{vframe}{Motivation: \pkg{mlr}}
  \begin{itemize}
    \item Unified interface for the basic building blocks: tasks, learners, resampling, hyperparameters, \ldots
    \item Reflections: nearly all objects are queryable (i.e.\ you can ask them for their properties and program on them)
    \item The OO-structure allows many generic algorithms:
      \begin{itemize}
        \item Bagging
        \item Stacking
        \item Feature Selection
        \item \ldots
      \end{itemize}
    \item Easily extensible via S3
      \begin{itemize}
        \item Explained in detail in the online tutorial
        \item You do not need to understand S3 to use \pkg{mlr}
      \end{itemize}
  \end{itemize}
\end{vframe}


<<gatherSummary,include=FALSE>>=
ee = as.environment("package:mlr")
nl = table(sub("^makeRLearner\\.([[:alpha:]]+)\\..+", "\\1", methods("makeRLearner")))
nm = sapply(list(classif = listMeasures("classif"), regr = listMeasures("regr"), surv = listMeasures("surv"), cluster = listMeasures("cluster")), length) - 4
@

\begin{vbframe}{What Learners are available?}
  \begin{scriptsize}
  \begin{columns}
    \column{0.5\textwidth}
    \begin{blocki}{Classification (\Sexpr{nl["classif"]})}
        \item LDA, QDA, RDA, MDA
        \item Trees and forests
        \item Boosting (different variants)
        \item SVMs (different variants)
        \item Deep Neural Networks
        \item \ldots
    \end{blocki}
    \begin{blocki}{Clustering (\Sexpr{nl["cluster"]})}
        \item K-Means
        \item EM
        \item DBscan
        \item X-Means
        \item \ldots
    \end{blocki}
    \column{0.4\textwidth}
    \begin{blocki}{Regression (\Sexpr{nl["regr"]})}
        \item Linear, lasso and ridge
        \item Boosting
        \item Trees and forests
        \item Gaussian processes
        \item Deep Neural Networks
        \item \ldots
    \end{blocki}
    \begin{blocki}{Survival (\Sexpr{nl["surv"]})}
        \item Cox-PH
        \item Cox-Boost
        \item Random survival forest
        \item Penalized regression
        \item \ldots
    \end{blocki}
  \end{columns}
  \end{scriptsize}
  \oneliner{We can explore them on the webpage -- or ask \pkg{mlr}}

  \framebreak
  
<<listlrns1, warning=FALSE, size='tiny'>>=
# list all classification learners which can predict probabilities
# and allow multiclass classification
listLearners("classif",
  properties = c("prob", "multiclass"))[1:5, 1:6]
@

% \framebreak

% \oneliner{Get all applicable learners for a task}
% <<listlrns2>>=
% listLearners(task)[1:5, c(-2, -5, -16)]
% @

\end{vbframe}

\begin{frame}{Building Blocks}
  \begin{center}
    \includegraphics[width=0.9\textwidth]{fig/ml_abstraction-crop.pdf}
  \end{center}
  \begin{itemize}
    \item \pkg{mlr} objects: tasks, learners, measures, resampling instances.
  \end{itemize}
\end{frame}

\begin{vframe}{Task Abstraction}
  \begin{itemize}
    \item Tasks encapsulate data and meta-information about it
    \item Regression, classification, clustering, survival tasks
  \end{itemize}
<<task1>>=
task = makeClassifTask(data = iris, target = "Species")
print(task)
@
\end{vframe}

\begin{vbframe}{Task Abstraction: API}
<<task2>>=
getTaskId(task)
str(getTaskData(task))
@
\framebreak
<<task3>>=
str(getTaskDesc(task))
@
\framebreak
<<task4>>=
getTaskSize(task)
getTaskFeatureNames(task)
getTaskTargetNames(task)
getTaskFormula(task)
summary(getTaskTargets(task))
@
\end{vbframe}

\begin{vframe}{Learner Abstraction}
  \begin{itemize}
    \item Internal structure of learners:
      \begin{itemize}
        \item wrappers around \code{fit()} and \code{predict()} of the specific package
        \item description of the parameter set, annotations, \ldots
      \end{itemize}
    \item Naming convention: \texttt{<tasktype>.<functionname>}\\
<<naming-convention,eval=FALSE>>=
makeLearner("classif.rpart")
makeLearner("regr.lm")
@
    \item Adding custom learners is covered in the tutorial
  \end{itemize}
% \framebreak
<<learner1>>=
lrn = makeLearner("classif.svm", predict.type = "prob", kernel = "linear", cost = 1)
print(lrn)
@
\end{vframe}

\begin{vframe}{Parameter Abstraction}
  \begin{itemize}
    \item Extensive meta-information for hyperparameters available:\\
      storage type, constraints, defaults, dependencies
    \item Automatically checked for feasibility
    \item You can program on parameters!
    \end{itemize}
<<parmset, size='tiny', echo=4>>=
w = getOption("width")
lrn = makeLearner("classif.svm", predict.type = "prob", kernel = "linear", cost = 1)
options(width = 160)
getParamSet(lrn)
options(width = w)
@
\end{vframe}

\begin{vbframe}{Basic Usage: Train/Predict/Evaluate}

<<tpe>>=
#Split data in train and test data
iris.train = iris[seq(1, 150, by = 2), ] # 1, 3, 5, 7, ... obs.
iris.test = iris[seq(2, 150, by = 2), ] # 2, 4, 6, 8, ... obs.

# create a task
task = makeClassifTask(data = iris.train, target = "Species")

# create a learner
lrn = makeLearner("classif.rpart")

# train the model
mod = train(lrn, task)

# predict the test data
pred = predict(mod, newdata = iris.test)

# evaluate performance of the model on the test data
performance(pred, mmce)
@

\end{vbframe}


\begin{vbframe}{Resampling Abstraction}

\includegraphics[width = \textwidth]{fig/Nested_Resampling.png}

\framebreak

  \begin{itemize}
    \item Procedure: Train, Predict, Eval, Repeat.
    \item Aim: Estimate expected model performance.
      \begin{itemize}
        \item Hold-Out
        \item Cross-validation (normal, repeated)
        \item Bootstrap (OOB, B632, B632+)
        \item Subsampling
        \item Stratification
        \item Blocking
      \end{itemize}
    \item Instantiate it or not (= create data split indices)
  \end{itemize}
<<resample1>>=
rdesc = makeResampleDesc("CV", iters = 3)
rin = makeResampleInstance(rdesc, task = task)
str(rin$train.inds)
@
  \framebreak
  \begin{blocki}{Resampling a learner}
    \item Measures on test (or train) sets
    \item Returns aggregated values, predictions and some useful extra information
<<resample2>>=
lrn = makeLearner("classif.rpart")
rdesc = makeResampleDesc("CV", iters = 3)
measures = list(mmce, timetrain)
r = resample(lrn, task, rdesc, measures = measures)
@
\item For the lazy
<<resample3, eval = FALSE>>=
r = crossval(lrn, task, iters = 3, measures = measures)
@
  \end{blocki}
\framebreak
<<resample2b>>=
print(r)
@
Container object: Measures (aggregated and for each test set), predictions, models, \dots
\end{vbframe}

\begin{vframe}{Performance Measures}
  \begin{itemize}
    \item Performance measures evaluate the predictions a test set and aggregate them over multiple in resampling iterations
    \item \Sexpr{nm["classif"]}~classification, \Sexpr{nm["regr"]}~regression,  \Sexpr{nm["cluster"]}~cluster, \Sexpr{nm["surv"]}~survival
    \item Adding custom measures is covered in the tutorial
\end{itemize}
<<measure>>=
print(mmce)
listMeasures("classif")[1:12]
@
\end{vframe}

\begin{vframe}{What measures are available?}
\oneliner{We can explore them on the webpage -- or ask \pkg{mlr}}
<<measure2>>=
listMeasures("classif")
listMeasures(task)
@
\end{vframe}

\begin{vbframe}{Benchmarking and Model Comparison}
  \begin{blocki}{Benchmarking}
    \item Comparison of multiple models on multiple data sets
    \item Aim: Find best learners for a data set or domain, learn about learner characteristics, \ldots
  \end{blocki}

<<echo = FALSE, results='hide'>>=
set.seed(12345)
@

<<eval=TRUE>>=
# these are predefined in mlr for toying around:
tasks = list(iris.task, sonar.task)
learners = list(
  makeLearner("classif.rpart"),
  makeLearner("classif.randomForest", ntree = 500),
  makeLearner("classif.svm")
)

rdesc = makeResampleDesc("CV", iters = 3)
br = benchmark(learners, tasks, rdesc)
@

Container object: Results, individual predictions, \dots

\framebreak

<<eval=TRUE, fig.height=4>>=
plotBMRBoxplots(br)
@

\end{vbframe}

\begin{vframe}{Automatic Model Selection}
  \begin{blocki}{Prior approaches:}
  \item Finding the unviversally best method\\
    $\leadsto$ Not found yet\\
  \item Exhaustive benchmarking / search \\
    $\leadsto$ Per data set: too expensive \\
    $\leadsto$ Over many: contradicting results
  \item Meta-Learning:\\
    $\leadsto$ No promising results yet \\
    $\leadsto$ Usually not for preprocessing / hyperparamters
  \end{blocki}

  \structure{Goal}: Data dependent + Automatic + Efficient
\end{vframe}

\begin{vframe}{Hyperparameter Tuning}
  \begin{blocki}{Tuning}
  \item Used to find \enquote{best} hyperparameters for a method in a data-dependent way
  \item General procedure: Tuner proposes param point, eval by resampling, feedback value to tuner
  \end{blocki}

  \begin{blocki}{Grid search}
  \item Basic method: Exhaustively try all combinations of finite grid\\
  $\leadsto$ Inefficient, combinatorial explosion, searches irrelevant areas
  \end{blocki}

  \begin{blocki}{Random search}
  \item Randomly draw parameters\\
  $\leadsto$ Scales better then grid search, easily extensible
  \end{blocki}
\end{vframe}


\begin{frame}{Adaptive tuning}
  \begin{center}
    \includegraphics[width=0.85\textwidth]{fig/ml_abstraction_optimization-crop.pdf}
  \end{center}
\end{frame}


\begin{vframe}{Tuning Example: Random Search}
<<echo = FALSE, include = FALSE>>=
library(rgenoud)
@
<<echo = TRUE, message=FALSE>>=
set.seed(1)
ps = makeParamSet(
  makeNumericParam("C", lower = -10, upper = 10, trafo = function(x) 2^x),
  makeNumericParam("sigma", lower = -10, upper = 10, trafo = function(x) 2^x)
)
ctrl = makeTuneControlRandom(maxit = 20L)
rdesc = makeResampleDesc("CV", iters = 2L)
res = tuneParams("classif.ksvm", task = pid.task, control = ctrl,
  resampling = rdesc, par.set = ps, show.info = FALSE)
res
@
\end{vframe}

\begin{frame}{mlrMBO: Model-Based Optimization Toolbox}
\begin{minipage}{0.4\linewidth}
    \begin{itemize}
      \item Any regression from mlr
      \item Arbtritrary infill
      \item Single - or multi-crit
      \item Multi-point proposal
      \item Via parallelMap and batchtools
        runs on many parallel backends and clusters
      \item Algorithm configuration
      \item Active research
    \end{itemize}

\end{minipage}
\begin{minipage}{0.55\linewidth}
    \includegraphics[width = \textwidth]{fig/mlrMBO1.pdf}
\end{minipage}
\begin{center}
    \begin{itemize}
      \item mlrMBO:
        \url{https://github.com/mlr-org/mlrMBO}
      \item mlrMBO Paper on arXiv (under review)
        \url{https://arxiv.org/abs/1703.03373}
    \end{itemize}
\end{center}
\end{frame}

\begin{vframe}{Tuning Example: MBO}
<<echo = TRUE>>=
library(mlrMBO)
ps = makeParamSet(
  makeNumericParam("C", lower = -10, upper = 10, trafo = function(x) 2^x),
  makeNumericParam("sigma", lower = -10, upper = 10, trafo = function(x) 2^x)
)
mbo.ctrl = setMBOControlInfill(makeMBOControl(), crit = crit.aei)
ctrl = makeTuneControlMBO(budget = 20L, mbo.control = mbo.ctrl)
rdesc = makeResampleDesc("CV", iters = 2L)
res = tuneParams("classif.ksvm", task = pid.task, control = ctrl,
  resampling = rdesc, par.set = ps, show.info = FALSE)
res
plot(res$mbo.result$final.opt.state)
@
\end{vframe}

\begin{vframe}{Tuning Example: mlrHyperopt}
  \begin{itemize}
    \item R-Package mlrHyperopt for effortless tuning
    \item Documentation: \url{http://jakob-r.github.io/mlrHyperopt}
    \item No knowledge of parameters needed.
    \item Decides automatically for suitable tuning method.
  \end{itemize}
<<echo = TRUE, warnings = FALSE>>=
library(mlrHyperopt)
res = hyperopt(task = pid.task, learner = "classif.ksvm")
res
@
\end{vframe}

\begin{vbframe}{Model Multiplexer}
The model multiplexer allows for tuning over multiple learners!
<<warning=FALSE, fig.width=8>>=
bls = list(
  makeLearner("classif.ksvm"),
  makeLearner("classif.randomForest")
)
lrn = makeModelMultiplexer(bls)
ps = makeModelMultiplexerParamSet(lrn,
  makeNumericParam("sigma", lower = -10, upper = 10, trafo = function(x) 2^x),
  makeNumericParam("C", lower = -10, upper = 10, trafo = function(x) 2^x),
  makeIntegerParam("mtry", lower = 1L, upper = 8L)
)
rdesc = makeResampleDesc("CV", iters = 2L)
ctrl = makeTuneControlIrace(maxExperiments = 120L)
res = tuneParams(lrn, pid.task, rdesc, par.set = ps, control = ctrl)
res
@
\end{vbframe}

\begin{vbframe}{\pkg{mlr} Learner Wrappers}
  \begin{blocki}{What?}
    \item Extend the functionality of learners by adding an \pkg{mlr} wrapper to them
    \item The wrapper hooks into the train and predict of the base learner and extends it
    \item This way, you can create a new \pkg{mlr} learner with extended functionality
    \item Hyperparameter definition spaces get joined!
  \end{blocki}
  \framebreak
  \begin{blocki}{Available Wrappers}
    \item \structure{Preprocessing}: PCA, normalization, dummy encoding, ...
    \item \structure{Parameter Tuning}: grid, optim, random search, genetic algorithms, CMAES, iRace, MBO
    \item \structure{Filter}: correlation- and entropy-based, $\mathcal{X}^2$-test, mRMR, \ldots
    \item \structure{Feature Selection}: (floating) sequential forward/backward, exhaustive search, genetic algorithms, \ldots
    \item \structure{Impute}: dummy variables, imputations with mean, median, min, max, empirical distribution or other learners
    \item \structure{Bagging} to fuse learners on bootstraped samples
    \item \structure{Stacking} to combine models in heterogenous ensembles
    \item \structure{Over- and Undersampling} for unbalanced classification
  \end{blocki}
\end{vbframe}

\begin{frame}{Moving on with mlr}
  \begin{minipage}{0.8\linewidth}
    \begin{itemize}
      \item Learn all details in the tutorial: \url{https://mlr-org.github.io/mlr/}
      \item Book a Machine Learning in R Course: \url{http://dortmunder-r-kurse.de/kurse/machine-learning-in-r/}
      \item Ask general questions in stackoverflow: \url{https://stackoverflow.com/questions/tagged/mlr}
      \item Found bugs? Report them: \url{https://github.com/mlr-org/mlr/issues}
      \item Want to contribute? Join our slack: \url{https://mlr-org.slack.com/}.
    \end{itemize}
  \end{minipage}
  \begin{minipage}{0.15\linewidth}
      \includegraphics[width = \textwidth]{fig/mlr.png}\\
      \includegraphics[width = \textwidth]{fig/r-kurs_logo.png}\\
      \includegraphics[width = \textwidth]{fig/stack_overflow.png}\\
      \includegraphics[width = \textwidth]{fig/GitHub-Mark.pdf}\\
  \end{minipage}
\end{frame}

\end{document}
% vim: set spelllang=en :
